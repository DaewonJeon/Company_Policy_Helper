# [í”„ë¡œì íŠ¸ 2] ì‚¬ë‚´ ê·œì •/ê³„ì•½ì„œ ê²€í†  AI ë¹„ì„œ: êµ¬í˜„ ì½”ë“œ

> **ëª©ì **: Python, LangChain, OpenAIë¥¼ í™œìš©í•˜ì—¬ ë‚´ ì»´í“¨í„°ì—ì„œ ëŒì•„ê°€ëŠ” **ì‚¬ë‚´ ê·œì • Q&A ì±—ë´‡**ì„ ë§Œë“­ë‹ˆë‹¤.
> **êµ¬ì¡°**: ë°ì´í„° ì£¼ì…ê¸°(`ingest.py`)ì™€ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤(`app.py`)ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.

---

## 1. í”„ë¡œì íŠ¸ í´ë” êµ¬ì¡° (Directory Structure)

ë°”íƒ•í™”ë©´ì— `ai_secretary` í´ë”ë¥¼ ë§Œë“¤ê³  ì•„ë˜ì™€ ê°™ì´ êµ¬ì„±í•˜ì„¸ìš”.

```text
ai_secretary/
â”œâ”€â”€ .env                  # API Key ì €ì¥ì†Œ
â”œâ”€â”€ requirements.txt      # í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª©ë¡
â”œâ”€â”€ ingest.py             # ê·œì •ì§‘ PDFë¥¼ ì½ì–´ì„œ DBì— ì €ì¥í•˜ëŠ” ì½”ë“œ (í•™ìŠµìš©)
â”œâ”€â”€ app.py                # ì±—ë´‡ í™”ë©´ ì‹¤í–‰ ì½”ë“œ (ì„œë¹„ìŠ¤ìš©)
â””â”€â”€ data/                 # PDF íŒŒì¼ì„ ë„£ì„ í´ë”
    â””â”€â”€ (ì—¬ê¸°ì— PDF íŒŒì¼ë“¤ì„ ë„£ìœ¼ì„¸ìš”)
```

---

## 2. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (requirements.txt)

ìµœì‹  AI ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

```text
langchain
langchain-community
langchain-openai
langchain-chroma
streamlit
pypdf
python-dotenv
openai
chromadb
tiktoken
```

---

## 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (.env)

`openai.com`ì—ì„œ ë°œê¸‰ë°›ì€ í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”. (ì¹´ë“œ ë“±ë¡ í•„ìš”, ì‚¬ìš©ëŸ‰ë§Œí¼ ê³¼ê¸ˆë˜ë‚˜ í…ŒìŠ¤íŠ¸ ì‹œ ëª‡ë°± ì› ìˆ˜ì¤€)

```env
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxxx
```

---

## 4. ë°ì´í„° í•™ìŠµê¸° êµ¬í˜„ (ingest.py)

ì´ ì½”ë“œëŠ” `data` í´ë”ì— ìˆëŠ” PDF íŒŒì¼ë“¤ì„ ì½ì–´ì„œ, AIê°€ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ **ë²¡í„° DB(Chroma)**ì— ì €ì¥í•©ë‹ˆë‹¤.
**ìµœì´ˆ 1íšŒ** ì‹¤í–‰í•˜ê±°ë‚˜, PDFê°€ ë°”ë€” ë•Œë§ˆë‹¤ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤.

```python
import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

# 1. í™˜ê²½ ì„¤ì • ë¡œë“œ
load_dotenv()

# ë°ì´í„° ê²½ë¡œ ì„¤ì •
DATA_PATH = "./data"
DB_PATH = "./chroma_db"  # ë²¡í„° DBê°€ ì €ì¥ë  í´ë”

def ingest_docs():
    print("ğŸ”„ ë¬¸ì„œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # 2. PDF íŒŒì¼ ë¡œë“œ
    documents = []
    if not os.path.exists(DATA_PATH):
        os.makedirs(DATA_PATH)
        print("âš ï¸ data í´ë”ê°€ ì—†ì–´ ìƒì„±í–ˆìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        return

    files = [f for f in os.listdir(DATA_PATH) if f.endswith('.pdf')]
    if not files:
        print("âš ï¸ data í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    for file in files:
        loader = PyPDFLoader(os.path.join(DATA_PATH, file))
        documents.extend(loader.load())
        print(f"   - {file} ë¡œë“œ ì™„ë£Œ ({len(documents)} í˜ì´ì§€)")

    # 3. í…ìŠ¤íŠ¸ ë¶„í•  (Chunking)
    # ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•´ 1000ì ë‹¨ìœ„ë¡œ ìë¥´ê³ , 200ìëŠ” ê²¹ì¹˜ê²Œ(overlap) ì„¤ì •
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    splits = text_splitter.split_documents(documents)
    print(f"ğŸ“Š ì´ {len(splits)}ê°œì˜ ì§€ì‹ ì¡°ê°ìœ¼ë¡œ ë¶„í•´ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # 4. ë²¡í„° DB ì €ì¥ (Embeddings)
    # OpenAIì˜ ì„ë² ë”© ëª¨ë¸(text-embedding-3-small) ì‚¬ìš© - ì €ë ´í•˜ê³  ì„±ëŠ¥ ì¢‹ìŒ
    print("ğŸ’¾ ë²¡í„° DBì— ì €ì¥ ì¤‘... (ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=OpenAIEmbeddings(model="text-embedding-3-small"),
        persist_directory=DB_PATH
    )
    
    print("âœ… í•™ìŠµ ì™„ë£Œ! 'chroma_db' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    ingest_docs()
```

---

## 5. ì±—ë´‡ UI êµ¬í˜„ (app.py)

ì‹¤ì œ ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ë˜ì§€ëŠ” í™”ë©´ì…ë‹ˆë‹¤. `ingest.py`ê°€ ë§Œë“¤ì–´ë‘” DBë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

```python
import streamlit as st
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# 1. í™˜ê²½ ì„¤ì • ë° DB ë¡œë“œ
load_dotenv()
st.set_page_config(page_title="ì‚¬ë‚´ ê·œì • AI ë¹„ì„œ", page_icon="ğŸ¤–")

DB_PATH = "./chroma_db"

@st.cache_resource
def load_rag_chain():
    # ì €ì¥ëœ DB ë¶ˆëŸ¬ì˜¤ê¸°
    if not os.path.exists(DB_PATH):
        return None
        
    vectorstore = Chroma(
        persist_directory=DB_PATH, 
        embedding_function=OpenAIEmbeddings(model="text-embedding-3-small")
    )
    retriever = vectorstore.as_retriever()
    
    # LLM (GPT-4o) ì„¤ì •
    llm = ChatOpenAI(model="gpt-4o", temperature=0) # ì˜¨ë„ë¥¼ 0ìœ¼ë¡œ í•´ì•¼ ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€

    # í”„ë¡¬í”„íŠ¸ ì„¤ê³„ (í˜ë¥´ì†Œë‚˜ ë¶€ì—¬)
    system_prompt = (
        "ë‹¹ì‹ ì€ íšŒì‚¬ì˜ ìœ ëŠ¥í•œ ê·œì • ë‹´ë‹¹ AI ë¹„ì„œì…ë‹ˆë‹¤. "
        "ì•„ë˜ ì œê³µëœ [Context]ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”. "
        "ë§Œì•½ ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì´ë¼ë©´ 'ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ê·œì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ì •ì§í•˜ê²Œ ë§í•˜ì„¸ìš”. "
        "ë‹µë³€ ëì—ëŠ” ë°˜ë“œì‹œ ì°¸ê³ í•œ ê·¼ê±° ë¬¸ì„œë¥¼ ëª…ì‹œí•˜ì„¸ìš”."
        "\n\n"
        "[Context]:\n{context}"
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])

    # ì²´ì¸ ì—°ê²°: ê²€ìƒ‰(Retriever) -> ë‹µë³€ìƒì„±(LLM)
    question_answer_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)
    
    return rag_chain

# UI êµ¬ì„±
st.title("ğŸ¤– ì‚¬ë‚´ ê·œì •/ê³„ì•½ì„œ AI ê²€í†  ë¹„ì„œ")
st.caption("ê¶ê¸ˆí•œ ê·œì •ì„ ë¬¼ì–´ë³´ì„¸ìš”. PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.")

if "messages" not in st.session_state:
    st.session_state.messages = []

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if question := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ê²½ì¡°ì‚¬ íœ´ê°€ëŠ” ë©°ì¹ ì¸ê°€ìš”?)"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": question})
    with st.chat_message("user"):
        st.markdown(question)

    # AI ë‹µë³€ ìƒì„±
    with st.chat_message("assistant"):
        rag_chain = load_rag_chain()
        
        if rag_chain is None:
            st.error("âš ï¸ í•™ìŠµëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € `ingest.py`ë¥¼ ì‹¤í–‰í•´ì„œ PDFë¥¼ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
        else:
            with st.spinner("ê·œì •ì§‘ì„ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                response = rag_chain.invoke({"input": question})
                answer = response["answer"]
                
                st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})
                
                # (ì„ íƒ) ê·¼ê±° ë¬¸ì„œ ë””ë²„ê¹…ìš© í‘œì‹œ
                with st.expander("ì°¸ê³ í•œ ë¬¸ì„œ ì¡°ê° ë³´ê¸°"):
                    for i, doc in enumerate(response["context"]):
                        st.markdown(f"**[ë¬¸ì„œ {i+1}] {doc.metadata.get('source', 'Unknown')}**")
                        st.text(doc.page_content[:200] + "...")
```
