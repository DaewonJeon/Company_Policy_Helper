# -*- coding: utf-8 -*-
import os
import sys
import io
import uuid

# Windows ì½˜ì†” UTF-8 ì¶œë ¥ ì„¤ì •
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import pdfplumber
from dotenv import load_dotenv
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document

# 1. í™˜ê²½ ì„¤ì • ë¡œë“œ
load_dotenv()

# ë°ì´í„° ê²½ë¡œ ì„¤ì •
DATA_PATH = "./data"
DB_PATH = "./chroma_db"  # ë²¡í„° DBê°€ ì €ì¥ë  í´ë”


def load_pdf_with_pdfplumber(file_path: str) -> list[Document]:
    """
    pdfplumberë¥¼ ì‚¬ìš©í•˜ì—¬ PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    PyPDFLoaderë³´ë‹¤ í•œêµ­ì–´ PDF íŒŒì‹±ì— ë” ì •í™•í•©ë‹ˆë‹¤.
    """
    documents = []
    with pdfplumber.open(file_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë ˆì´ì•„ì›ƒ ê¸°ë°˜)
            text = page.extract_text()
            if text:
                # ë¶ˆí•„ìš”í•œ ë‹¤ì¤‘ ê³µë°± ì •ë¦¬
                text = ' '.join(text.split())
                documents.append(Document(
                    page_content=text,
                    metadata={
                        "source": file_path,
                        "page": page_num + 1
                    },
                    id=str(uuid.uuid4())
                ))
    return documents


def ingest_docs():
    print("ğŸ”„ ë¬¸ì„œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # 2. PDF íŒŒì¼ ë¡œë“œ (pdfplumber ì‚¬ìš©)
    documents = []
    if not os.path.exists(DATA_PATH):
        os.makedirs(DATA_PATH)
        print("âš ï¸ data í´ë”ê°€ ì—†ì–´ ìƒì„±í–ˆìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        return

    files = [f for f in os.listdir(DATA_PATH) if f.endswith('.pdf')]
    if not files:
        print("âš ï¸ data í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    for file in files:
        file_path = os.path.join(DATA_PATH, file)
        docs = load_pdf_with_pdfplumber(file_path)
        documents.extend(docs)
        print(f"   - {file} ë¡œë“œ ì™„ë£Œ ({len(docs)} í˜ì´ì§€)")

    # 3. í…ìŠ¤íŠ¸ ë¶„í•  (Chunking)
    # ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•´ 1000ì ë‹¨ìœ„ë¡œ ìë¥´ê³ , 200ìëŠ” ê²¹ì¹˜ê²Œ(overlap) ì„¤ì •
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    splits = text_splitter.split_documents(documents)
    print(f"ğŸ“Š ì´ {len(splits)}ê°œì˜ ì§€ì‹ ì¡°ê°ìœ¼ë¡œ ë¶„í•´ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # 4. ë²¡í„° DB ì €ì¥ (Embeddings)
    # OpenAIì˜ ì„ë² ë”© ëª¨ë¸(text-embedding-3-small) ì‚¬ìš© - ì €ë ´í•˜ê³  ì„±ëŠ¥ ì¢‹ìŒ
    print("ğŸ’¾ ë²¡í„° DBì— ì €ì¥ ì¤‘... (ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    
    # ê° ë¬¸ì„œ ì¡°ê°ì— ê³ ìœ  ID ìƒì„±
    ids = [str(uuid.uuid4()) for _ in splits]
    
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=OpenAIEmbeddings(model="text-embedding-3-small"),
        persist_directory=DB_PATH,
        ids=ids
    )
    
    print("âœ… í•™ìŠµ ì™„ë£Œ! 'chroma_db' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    ingest_docs()
