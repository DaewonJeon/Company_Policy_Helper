import streamlit as st
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate


# 1. í™˜ê²½ ì„¤ì • ë° DB ë¡œë“œ
load_dotenv()
st.set_page_config(page_title="ì‚¬ë‚´ ê·œì • AI ë¹„ì„œ", page_icon="ğŸ¤–")

DB_PATH = "./chroma_db"

@st.cache_resource
def load_rag_chain():
    # ì €ì¥ëœ DB ë¶ˆëŸ¬ì˜¤ê¸°
    if not os.path.exists(DB_PATH):
        return None
        
    vectorstore = Chroma(
        persist_directory=DB_PATH, 
        embedding_function=OpenAIEmbeddings(model="text-embedding-3-small")
    )
    retriever = vectorstore.as_retriever()
    
    # LLM (GPT-4o) ì„¤ì •
    llm = ChatOpenAI(model="gpt-4o", temperature=0) # ì˜¨ë„ë¥¼ 0ìœ¼ë¡œ í•´ì•¼ ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€

    # í”„ë¡¬í”„íŠ¸ ì„¤ê³„ (í˜ë¥´ì†Œë‚˜ ë¶€ì—¬)
    system_prompt = (
        "ë‹¹ì‹ ì€ íšŒì‚¬ì˜ ìœ ëŠ¥í•œ ê·œì • ë‹´ë‹¹ AI ë¹„ì„œì…ë‹ˆë‹¤. "
        "ì•„ë˜ ì œê³µëœ [Context]ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”. "
        "ë§Œì•½ ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì´ë¼ë©´ 'ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ê·œì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ì •ì§í•˜ê²Œ ë§í•˜ì„¸ìš”. "
        "ë‹µë³€ ëì—ëŠ” ë°˜ë“œì‹œ ì°¸ê³ í•œ ê·¼ê±° ë¬¸ì„œë¥¼ ëª…ì‹œí•˜ì„¸ìš”."
        "\n\n"
        "[Context]:\n{context}"
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])

    # ì²´ì¸ ì—°ê²°: ê²€ìƒ‰(Retriever) -> ë‹µë³€ìƒì„±(LLM)
    question_answer_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)
    
    return rag_chain

# UI êµ¬ì„±
st.title("ğŸ¤– ì‚¬ë‚´ ê·œì •/ê³„ì•½ì„œ AI ê²€í†  ë¹„ì„œ")
st.caption("ê¶ê¸ˆí•œ ê·œì •ì„ ë¬¼ì–´ë³´ì„¸ìš”. PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.")

if "messages" not in st.session_state:
    st.session_state.messages = []

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if question := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ê²½ì¡°ì‚¬ íœ´ê°€ëŠ” ë©°ì¹ ì¸ê°€ìš”?)"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": question})
    with st.chat_message("user"):
        st.markdown(question)

    # AI ë‹µë³€ ìƒì„±
    with st.chat_message("assistant"):
        rag_chain = load_rag_chain()
        
        if rag_chain is None:
            st.error("âš ï¸ í•™ìŠµëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € `ingest.py`ë¥¼ ì‹¤í–‰í•´ì„œ PDFë¥¼ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
        else:
            with st.spinner("ê·œì •ì§‘ì„ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                response = rag_chain.invoke({"input": question})
                answer = response["answer"]
                
                st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})
                
                # (ì„ íƒ) ê·¼ê±° ë¬¸ì„œ ë””ë²„ê¹…ìš© í‘œì‹œ
                with st.expander("ì°¸ê³ í•œ ë¬¸ì„œ ì¡°ê° ë³´ê¸°"):
                    for i, doc in enumerate(response["context"]):
                        st.markdown(f"**[ë¬¸ì„œ {i+1}] {doc.metadata.get('source', 'Unknown')}**")
                        st.text(doc.page_content[:200] + "...")
