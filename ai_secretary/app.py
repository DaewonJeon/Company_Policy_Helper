import streamlit as st
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough


# 1. í™˜ê²½ ì„¤ì • ë° DB ë¡œë“œ
load_dotenv()
st.set_page_config(page_title="ì‚¬ë‚´ ê·œì • AI ë¹„ì„œ", page_icon="ğŸ¤–")

DB_PATH = "./chroma_db"

# ëª¨ë¸ ì •ë³´ ì •ì˜
MODEL_OPTIONS = {
    "GPT-4o (OpenAI)": {
        "provider": "openai",
        "model": "gpt-4o",
        "description": "OpenAI ìµœì‹  ë©€í‹°ëª¨ë‹¬ ëª¨ë¸, ë¹ ë¥´ê³  ì •í™•í•¨"
    },
    "GPT-4o-mini (OpenAI)": {
        "provider": "openai",
        "model": "gpt-4o-mini",
        "description": "GPT-4o ê²½ëŸ‰ ë²„ì „, ë” ì €ë ´í•˜ê³  ë¹ ë¦„"
    },
    "Gemini 2.5 Flash (Google)": {
        "provider": "google",
        "model": "models/gemini-2.5-flash",
        "description": "ë¹ ë¥¸ ì‘ë‹µ, ë¹„ìš© íš¨ìœ¨ì "
    },
    "Gemini 2.5 Pro (Google)": {
        "provider": "google",
        "model": "models/gemini-2.5-pro",
        "description": "ë³µì¡í•œ ì¶”ë¡  ë° ì½”ë”© ì‘ì—…ì— ì í•© (ë¬´ë£Œ í‹°ì–´ ê°€ëŠ¥)"
    },
    "Gemini 3 Flash Preview (Google)": {
        "provider": "google",
        "model": "models/gemini-3-flash-preview",
        "description": "ìµœì‹  ëª¨ë¸, í–¥ìƒëœ ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥"
    },
}


def get_llm(model_name: str):
    """ì„ íƒëœ ëª¨ë¸ì— ë§ëŠ” LLM ê°ì²´ ë°˜í™˜"""
    model_info = MODEL_OPTIONS[model_name]
    
    if model_info["provider"] == "openai":
        return ChatOpenAI(model=model_info["model"], temperature=0)
    elif model_info["provider"] == "google":
        return ChatGoogleGenerativeAI(model=model_info["model"], temperature=0)


def load_vectorstore():
    """ë²¡í„° DB ë¡œë“œ"""
    if not os.path.exists(DB_PATH):
        return None
    
    return Chroma(
        persist_directory=DB_PATH,
        embedding_function=OpenAIEmbeddings(model="text-embedding-3-small")
    )


def format_docs(docs):
    """ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í¬ë§·"""
    return "\n\n".join(doc.page_content for doc in docs)


def create_rag_chain(llm, retriever):
    """LCEL ë°©ì‹ìœ¼ë¡œ RAG ì²´ì¸ ìƒì„±"""
    # í”„ë¡¬í”„íŠ¸ ì„¤ê³„
    system_prompt = """ë‹¹ì‹ ì€ íšŒì‚¬ì˜ ìœ ëŠ¥í•œ ë¬¸ì„œ ê²€í†  AI ë¹„ì„œì…ë‹ˆë‹¤.
ì•„ë˜ ì œê³µëœ [Context]ë¥¼ ê¼¼ê¼¼íˆ ì½ê³ , ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
Contextì— ê´€ë ¨ ë‚´ìš©ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ê·¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸íˆ ë‹µë³€í•˜ì„¸ìš”.
Contextì— ê´€ë ¨ ë‚´ìš©ì´ ì „í˜€ ì—†ëŠ” ê²½ìš°ì—ë§Œ 'ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë§í•˜ì„¸ìš”.
ë‹µë³€ ëì—ëŠ” ì°¸ê³ í•œ ë¬¸ì„œ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.

[Context]:
{context}"""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{question}"),
    ])

    # LCEL ì²´ì¸ êµ¬ì„±
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain


# ==================== UI êµ¬ì„± ====================

st.title("ğŸ¤– ì‚¬ë‚´ ê·œì •/ê³„ì•½ì„œ AI ê²€í†  ë¹„ì„œ")
st.caption("ê¶ê¸ˆí•œ ê·œì •ì„ ë¬¼ì–´ë³´ì„¸ìš”. PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.")

# ì‚¬ì´ë“œë°”: ëª¨ë¸ ì„ íƒ
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    selected_model = st.selectbox(
        "AI ëª¨ë¸ ì„ íƒ",
        options=list(MODEL_OPTIONS.keys()),
        index=0,
        help="ë‹µë³€ì„ ìƒì„±í•  AI ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”"
    )
    
    # ì„ íƒëœ ëª¨ë¸ ì •ë³´ í‘œì‹œ
    model_info = MODEL_OPTIONS[selected_model]
    st.caption(f"ğŸ“ {model_info['description']}")
    
    # Google ëª¨ë¸ ì„ íƒ ì‹œ API í‚¤ í™•ì¸ ì•ˆë‚´
    if model_info["provider"] == "google":
        if not os.getenv("GOOGLE_API_KEY"):
            st.warning("âš ï¸ GOOGLE_API_KEYê°€ .envì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    st.divider()
    
    # ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì‚­ì œ", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if question := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ê²½ì¡°ì‚¬ íœ´ê°€ëŠ” ë©°ì¹ ì¸ê°€ìš”?)"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": question})
    with st.chat_message("user"):
        st.markdown(question)

    # AI ë‹µë³€ ìƒì„±
    with st.chat_message("assistant"):
        vectorstore = load_vectorstore()
        
        if vectorstore is None:
            st.error("âš ï¸ í•™ìŠµëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € `ingest.py`ë¥¼ ì‹¤í–‰í•´ì„œ PDFë¥¼ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
        else:
            try:
                with st.spinner(f"ğŸ“š {selected_model}ë¡œ ê²€ìƒ‰ ì¤‘..."):
                    # ì„ íƒëœ ëª¨ë¸ë¡œ LLM ìƒì„±
                    llm = get_llm(selected_model)
                    retriever = vectorstore.as_retriever()
                    rag_chain = create_rag_chain(llm, retriever)
                    
                    # RAG ì²´ì¸ ì‹¤í–‰
                    answer = rag_chain.invoke(question)
                    
                    st.markdown(answer)
                    st.session_state.messages.append({"role": "assistant", "content": answer})
                    
                    # ê·¼ê±° ë¬¸ì„œ í‘œì‹œ
                    with st.expander("ì°¸ê³ í•œ ë¬¸ì„œ ì¡°ê° ë³´ê¸°"):
                        docs = retriever.invoke(question)
                        for i, doc in enumerate(docs):
                            st.markdown(f"**[ë¬¸ì„œ {i+1}] {doc.metadata.get('source', 'Unknown')}**")
                            st.text(doc.page_content[:200] + "...")
            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                st.info("ğŸ’¡ API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
